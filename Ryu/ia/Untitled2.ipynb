{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24012,
     "status": "ok",
     "timestamp": 1699717969889,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "ZUzpKH4GKDrT",
    "outputId": "f5e1532e-5110-47db-9b45-6da4823df93b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 01:03:36.707309: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from pyDeepInsight import ImageTransformer, LogScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import pickle, joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1699717970569,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "EkF1hfMSKVW9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('FlowStatsfiles.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1699717970571,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "-mqVmnGSO-_B",
    "outputId": "7ed34302-2da8-483d-f5ba-ca3d45ee305f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datapath</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>in_port</th>\n",
       "      <th>eth_type</th>\n",
       "      <th>eth_src</th>\n",
       "      <th>eth_dst</th>\n",
       "      <th>ip_src</th>\n",
       "      <th>tp_src</th>\n",
       "      <th>ip_dst</th>\n",
       "      <th>...</th>\n",
       "      <th>protocole</th>\n",
       "      <th>pktcount</th>\n",
       "      <th>bytecount</th>\n",
       "      <th>flowdur_sec</th>\n",
       "      <th>flowdur_nsec</th>\n",
       "      <th>pktcount_sec</th>\n",
       "      <th>pktcount_nsec</th>\n",
       "      <th>bytecount_sec</th>\n",
       "      <th>bytecount_nsec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.20172.24.16.10</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>06:ad:7d:c1:dc:1b</td>\n",
       "      <td>0a:b1:b5:23:aa:ad</td>\n",
       "      <td>172.24.16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.1</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>371000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.30172.24.16.40</td>\n",
       "      <td>5</td>\n",
       "      <td>2048</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>769000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>5.981795e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.40172.24.16.30</td>\n",
       "      <td>3</td>\n",
       "      <td>2048</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>758000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.068602e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>172.24.16.30172.24.16.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2048</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>757000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.076618e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>172.24.16.40172.24.16.30</td>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>752000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.117021e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp  datapath                   flow_id  in_port  eth_type  \\\n",
       "0  1.702643e+09         5  172.24.16.20172.24.16.10        1      2048   \n",
       "1  1.702643e+09         5  172.24.16.30172.24.16.40        5      2048   \n",
       "2  1.702643e+09         5  172.24.16.40172.24.16.30        3      2048   \n",
       "3  1.702643e+09         2  172.24.16.30172.24.16.40        3      2048   \n",
       "4  1.702643e+09         2  172.24.16.40172.24.16.30        2      2048   \n",
       "\n",
       "             eth_src            eth_dst       ip_src  tp_src       ip_dst  \\\n",
       "0  06:ad:7d:c1:dc:1b  0a:b1:b5:23:aa:ad  172.24.16.2       0  172.24.16.1   \n",
       "1  a2:5c:46:f2:34:27  20:77:6d:b7:70:8a  172.24.16.3       0  172.24.16.4   \n",
       "2  20:77:6d:b7:70:8a  a2:5c:46:f2:34:27  172.24.16.4       0  172.24.16.3   \n",
       "3  a2:5c:46:f2:34:27  20:77:6d:b7:70:8a  172.24.16.3       0  172.24.16.4   \n",
       "4  20:77:6d:b7:70:8a  a2:5c:46:f2:34:27  172.24.16.4       0  172.24.16.3   \n",
       "\n",
       "   ...  protocole  pktcount  bytecount flowdur_sec  flowdur_nsec  \\\n",
       "0  ...       ICMP         1         98           0     371000000   \n",
       "1  ...       ICMP        46       4508          63     769000000   \n",
       "2  ...       ICMP        46       4508          63     758000000   \n",
       "3  ...       ICMP        46       4508          63     757000000   \n",
       "4  ...       ICMP        46       4508          63     752000000   \n",
       "\n",
       "   pktcount_sec  pktcount_nsec  bytecount_sec  bytecount_nsec  label  \n",
       "0      0.000000   0.000000e+00       0.000000        0.000000      0  \n",
       "1      0.730159   5.981795e-08      71.555556        0.000006      0  \n",
       "2      0.730159   6.068602e-08      71.555556        0.000006      0  \n",
       "3      0.730159   6.076618e-08      71.555556        0.000006      0  \n",
       "4      0.730159   6.117021e-08      71.555556        0.000006      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136766 entries, 0 to 136765\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   timestamp       136766 non-null  float64\n",
      " 1   datapath        136766 non-null  int64  \n",
      " 2   flow_id         136766 non-null  object \n",
      " 3   in_port         136766 non-null  int64  \n",
      " 4   eth_type        136766 non-null  int64  \n",
      " 5   eth_src         136766 non-null  object \n",
      " 6   eth_dst         136766 non-null  object \n",
      " 7   ip_src          136766 non-null  object \n",
      " 8   tp_src          136766 non-null  int64  \n",
      " 9   ip_dst          136766 non-null  object \n",
      " 10  tp_dst          136766 non-null  int64  \n",
      " 11  icmp_code       136766 non-null  int64  \n",
      " 12  icmp_type       136766 non-null  int64  \n",
      " 13  protocole       136766 non-null  object \n",
      " 14  pktcount        136766 non-null  int64  \n",
      " 15  bytecount       136766 non-null  int64  \n",
      " 16  flowdur_sec     136766 non-null  int64  \n",
      " 17  flowdur_nsec    136766 non-null  int64  \n",
      " 18  pktcount_sec    136766 non-null  float64\n",
      " 19  pktcount_nsec   136766 non-null  float64\n",
      " 20  bytecount_sec   136766 non-null  float64\n",
      " 21  bytecount_nsec  136766 non-null  float64\n",
      " 22  label           136766 non-null  int64  \n",
      "dtypes: float64(5), int64(12), object(6)\n",
      "memory usage: 24.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1699717970573,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "kN4z-xCKPHGh"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop(['timestamp','datapath','flow_id','in_port','eth_type','eth_src', 'eth_dst', 'ip_src', 'ip_dst', 'protocole', 'icmp_code', 'icmp_type', 'flowdur_sec', 'flowdur_nsec'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1699717971090,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "Rw9mKgcTPHTg"
   },
   "outputs": [],
   "source": [
    "df.protocole.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['label']\n",
    "del df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1699717971091,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "zRI5c5zhPHgd",
    "outputId": "da278e6c-46cc-40e9-db89-d5569cfbd15f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp_src</th>\n",
       "      <th>tp_dst</th>\n",
       "      <th>pktcount</th>\n",
       "      <th>bytecount</th>\n",
       "      <th>pktcount_sec</th>\n",
       "      <th>pktcount_nsec</th>\n",
       "      <th>bytecount_sec</th>\n",
       "      <th>bytecount_nsec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.083276e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.686728e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.229505e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.711205e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.276371e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.713466e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.280699e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.724859e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.302512e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tp_src  tp_dst  pktcount     bytecount  pktcount_sec  pktcount_nsec  \\\n",
       "0     0.0     0.0  0.000002  4.083276e-09      0.000000   0.000000e+00   \n",
       "1     0.0     0.0  0.000079  1.878307e-07      0.000006   1.686728e-07   \n",
       "2     0.0     0.0  0.000079  1.878307e-07      0.000006   1.711205e-07   \n",
       "3     0.0     0.0  0.000079  1.878307e-07      0.000006   1.713466e-07   \n",
       "4     0.0     0.0  0.000079  1.878307e-07      0.000006   1.724859e-07   \n",
       "\n",
       "   bytecount_sec  bytecount_nsec  label  \n",
       "0   0.000000e+00    0.000000e+00    0.0  \n",
       "1   2.229147e-08    3.229505e-10    0.0  \n",
       "2   2.229147e-08    3.276371e-10    0.0  \n",
       "3   2.229147e-08    3.280699e-10    0.0  \n",
       "4   2.229147e-08    3.302512e-10    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "df_nrm= scaler.fit_transform(df)\n",
    "df_nv = pd.DataFrame(df_nrm, columns=df.columns)\n",
    "df_nv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1699717971091,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "eS-hehCVPHr1"
   },
   "outputs": [],
   "source": [
    "#X = df_nv.iloc[:,1:17].astype(float)\n",
    "#y = df_nv.iloc[:,-1]\n",
    "# X = df_nv.drop('label', axis=1).astype(float)\n",
    "# y = df_nv['label']\n",
    "X = df.drop('label', axis=1).astype(float)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1699717971092,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "g-GT8Gp6QVTE"
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgTTVUBxQebM",
    "outputId": "b6789956-fd89-406f-d53c-a91f98ad957e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "PrÃ©cision sur l'ensemble de validation : 99.56%\n",
      "PrÃ©cision sur l'ensemble de test : 99.6%\n",
      "Meilleurs hyperparamÃ¨tres: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10232\n",
      "           1       1.00      0.99      1.00     10283\n",
      "\n",
      "    accuracy                           1.00     20515\n",
      "   macro avg       1.00      1.00      1.00     20515\n",
      "weighted avg       1.00      1.00      1.00     20515\n",
      "\n",
      "1.18 Minute(s) --- temps pour le modÃ¨le KNN \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KNN.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# GridSearchCV pour trouver les meilleurs hyperparamÃ¨tres\n",
    "knnc = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knnc_search = GridSearchCV(knnc, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy', verbose=2)\n",
    "knnc_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = knnc_search.best_params_\n",
    "n_neighbors = best_params['n_neighbors']\n",
    "weights = best_params['weights']\n",
    "metric = best_params['metric']\n",
    "\n",
    "# Utilisation des meilleurs hyperparamÃ¨tres sur l'ensemble d'entraÃ®nement\n",
    "KNN = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, weights=weights)\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction et Ã©valuation sur l'ensemble de validation\n",
    "predicted_val = KNN.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"PrÃ©cision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# PrÃ©diction et Ã©valuation sur l'ensemble de test\n",
    "predicted_test = KNN.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "print(f\"PrÃ©cision sur l'ensemble de test : {round(accuracy_test * 100, 2)}%\")\n",
    "\n",
    "# Affichage des rÃ©sultats finaux\n",
    "print(\"Meilleurs hyperparamÃ¨tres:\", best_params)\n",
    "print(classification_report(predicted_test, y_test))\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minute(s) --- temps pour le modÃ¨le KNN \")\n",
    "\n",
    "    \n",
    "joblib.dump(KNN, 'KNN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMj3aTZUYiEo"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Liste pour stocker les prÃ©cisions pour chaque noyau\n",
    "accuracy_list = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    # CrÃ©ation du modÃ¨le SVM avec le noyau actuel\n",
    "    SVM = svm.SVC(kernel=kernel)\n",
    "\n",
    "    # EntraÃ®nement du modÃ¨le\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    # PrÃ©diction et Ã©valuation sur l'ensemble de validation\n",
    "    predicted_svm = SVM.predict(X_val)\n",
    "    accuracy_val = accuracy_score(y_val, predicted_svm)\n",
    "    print(f\"PrÃ©cision sur l'ensemble de validation du noyau {kernel} : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "    #PrÃ©diction et Ã©valuation sur l'ensemble de test\n",
    "    predicted_test = SVM.predict(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "    accuracy_list.append(accuracy_test)\n",
    "    print(f\"PrÃ©cision sur l'ensemble de test du noyau {kernel} : {round(accuracy_test * 100, 2)}%\")\n",
    "    print(\"########################################################################\")\n",
    "\n",
    "# SÃ©lection du meilleur noyau en fonction de la prÃ©cision maximale\n",
    "best_kernel = kernels[np.argmax(accuracy_list)]\n",
    "\n",
    "# EntraÃ®nement final avec le meilleur noyau\n",
    "best_SVM = svm.SVC(kernel=best_kernel)\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction avec le modÃ¨le optimal\n",
    "predicted_svm = best_SVM.predict(X_test)\n",
    "\n",
    "# Calcul de la prÃ©cision\n",
    "accuracy_svm = accuracy_score(y_test, predicted_svm)\n",
    "\n",
    "# Affichage des rÃ©sultats finaux\n",
    "print(f\"PrÃ©cision du modÃ¨le SVM avec le meilleur noyau (noyau {best_kernel}): {round(accuracy_svm * 100, 2)}%\")\n",
    "print(\"########################################################################\")\n",
    "print('Le meilleur noyau est:', best_kernel)\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(y_test, predicted_svm))\n",
    "print(\"########################################################################\")\n",
    "print(\"--- %s seconds --- temps pour le model SVM\" % (time.time() - start_time))\n",
    "\n",
    "# Sauvegarde du modÃ¨le avec pickle\n",
    "model_serialized = pickle.dumps(best_SVM)\n",
    "with open('modele_svm.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "\n",
    "joblib.dump(best_SVM, 'best_SVM.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNMHRu51ioTy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# DÃ©finissez la grille d'hyperparamÃ¨tres Ã  explorer\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# CrÃ©ez le modÃ¨le RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Utilisez GridSearchCV pour trouver les meilleurs hyperparamÃ¨tres\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleurs hyperparamÃ¨tres trouvÃ©s\n",
    "rf_classifier_best = grid_search.best_estimator_\n",
    "\n",
    "# EntraÃ®nez un modÃ¨le RandomForest avec les meilleurs hyperparamÃ¨tres\n",
    "rf_classifier_best.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction et Ã©valuation sur l'ensemble de validation\n",
    "predicted_val = rf_classifier_best.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"PrÃ©cision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# PrÃ©disez les Ã©tiquettes\n",
    "predicted_rf = rf_classifier_best.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, predicted_rf)\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print(f\"Random Forest PrÃ©cision: {round(accuracy_rf * 100, 2)}%\")\n",
    "print(classification_report(predicted_rf, y_test))\n",
    "print(\"--- %s secondes --- temps pour le Random Forest \" % (time.time() - start_time))\n",
    "\n",
    "# Sauvegarde du modÃ¨le avec pickle\n",
    "model_serialized = pickle.dumps(rf_classifier_best)\n",
    "with open('modele_rf.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(rf_classifier_best, 'rf_classifier_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V2F7WcuRmzQi",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aizen/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/aizen/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/aizen/anaconda3/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrÃ©cision sur l'ensemble de validation : 99.28%\n",
      "PrÃ©cision: 99.29%\n",
      "\n",
      "########################################################################\n",
      "Meilleur solveur : lbfgs\n",
      "########################################################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     10251\n",
      "           1       1.00      0.99      0.99     10264\n",
      "\n",
      "    accuracy                           0.99     20515\n",
      "   macro avg       0.99      0.99      0.99     20515\n",
      "weighted avg       0.99      0.99      0.99     20515\n",
      " \n",
      "\n",
      "########################################################################\n",
      "28.74 Minutes --- temps pour la rÃ©gression logistique \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aizen/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ParamÃ¨tres du modÃ¨le\n",
    "C_value = 0.03\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "results_lr = []\n",
    "\n",
    "start_time = time.time()\n",
    "best_accuracy = 0\n",
    "best_solver = None\n",
    "\n",
    "for solver in solvers:\n",
    "    # EntraÃ®nement du modÃ¨le\n",
    "    logreg_model = LogisticRegression(C=C_value, solver=solver, max_iter=1000, tol=1e-3).fit(X_train, y_train)\n",
    "    predicted_lr = logreg_model.predict(X_test)\n",
    "    accuracy_lr = accuracy_score(y_test, predicted_lr)\n",
    "    \n",
    "    results_lr.append({\n",
    "        'solver': solver,\n",
    "        'accuracy': f'{accuracy_lr * 100:.2f}%',\n",
    "        'coefficients': {'W': logreg_model.coef_, 'b': logreg_model.intercept_}\n",
    "    })\n",
    "\n",
    "    if accuracy_lr > best_accuracy:\n",
    "        best_accuracy = accuracy_lr\n",
    "        best_solver = solver\n",
    "\n",
    "# EntraÃ®nement final avec le meilleur solveur\n",
    "logreg_model = LogisticRegression(C=C_value, solver=best_solver).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PrÃ©diction et Ã©valuation sur l'ensemble de validation\n",
    "predicted_val = logreg_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"PrÃ©cision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# PrÃ©disez les Ã©tiquettes\n",
    "\n",
    "predicted_lr = logreg_model.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, predicted_lr)\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print(f\"PrÃ©cision: {accuracy_lr * 100:.2f}%\\n\")\n",
    "print(\"########################################################################\")\n",
    "print('Meilleur solveur :', best_solver)\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(predicted_lr, y_test), '\\n')\n",
    "print(\"########################################################################\")\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minutes --- temps pour la rÃ©gression logistique \")\n",
    "#print(f\"--- {(time.time() - start_time:.2f)} Minutes --- temps pour la rÃ©gression logistique\")\n",
    "\n",
    "# Sauvegarde du modÃ¨le avec pickle\n",
    "model_serialized = pickle.dumps(logreg_model)\n",
    "with open('modele_lr.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(logreg_model, 'logreg_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "PrÃ©cision sur l'ensemble de validation : 99.43%\n",
      "criterion: gini, max depth: None, max_leaf: 5\n",
      "La prÃ©cision est : 99.5%\n",
      "########################################################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10251\n",
      "           1       1.00      0.99      1.00     10264\n",
      "\n",
      "    accuracy                           1.00     20515\n",
      "   macro avg       1.00      1.00      1.00     20515\n",
      "weighted avg       1.00      1.00      1.00     20515\n",
      "\n",
      "########################################################################\n",
      "0.26 Minutes secondes --- temps pour l'arbre de dÃ©cision \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dt_classifier.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Recherche des hyperparamÃ¨tres optimaux\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'max_leaf_nodes': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "dt_search = GridSearchCV(dt_classifier, param_grid=param_grid, n_jobs=-1, cv=5, scoring='accuracy', verbose=2)\n",
    "dt_search.fit(X_train, y_train)\n",
    "\n",
    "criterion = dt_search.best_params_['criterion']\n",
    "max_depth = dt_search.best_params_['max_depth']\n",
    "max_leaf_nodes = dt_search.best_params_['max_leaf_nodes']\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le avec les hyperparamÃ¨tres optimaux\n",
    "dt_classifier = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# PrÃ©diction et Ã©valuation sur l'ensemble de validation\n",
    "predicted_val = dt_classifier.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"PrÃ©cision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "\n",
    "# PrÃ©diction et Ã©valuation du modÃ¨le\n",
    "predicted_dt = dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, predicted_dt)\n",
    "\n",
    "# Stockage des rÃ©sultats\n",
    "results = {\n",
    "    'criterion': criterion,\n",
    "    'max_depth': max_depth,\n",
    "    'max_leaf_nodes': max_leaf_nodes,\n",
    "    'accuracy': round(accuracy_dt * 100, 2)\n",
    "}\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print(f\"criterion: {criterion}, max depth: {max_depth}, max_leaf: {max_leaf_nodes}\")\n",
    "print(f\"La prÃ©cision est : {results['accuracy']}%\")\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(predicted_dt, y_test))\n",
    "print(\"########################################################################\")\n",
    "\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minutes secondes --- temps pour l'arbre de dÃ©cision \")\n",
    "\n",
    "# Sauvegarde du modÃ¨le avec pickle\n",
    "model_serialized = pickle.dumps(dt_classifier)\n",
    "with open('modele_dt.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(dt_classifier, 'dt_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation du modÃ¨le CNN 1D\n",
    "model = models.Sequential()\n",
    "\n",
    "# Ajout d'une couche de convolution 1D avec activation ReLU et pooling\n",
    "model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(15, 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Ajout d'une deuxiÃ¨me couche de convolution 1D avec activation ReLU et pooling\n",
    "model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Aplatir les donnÃ©es pour les passer Ã  une couche dense (fully connected)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Ajout d'une couche fully connected avec activation ReLU\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Utilisez 'softmax' si vous avez plus de deux classes\n",
    "\n",
    "# Compilation du modÃ¨le\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Utilisez 'categorical_crossentropy' si vous avez plus de deux classes\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Afficher un rÃ©sumÃ© du modÃ¨le\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un rappel qui arrÃªte la formation lorsqu'il n'y aura pas d'amÃ©lioration de la perte de validation pendant 5 Ã©poques consÃ©cutives.\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True)\n",
    "              \n",
    "# EntraÃ®nement du modÃ¨le avec ensemble de validation\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# Ã‰valuation du modÃ¨le sur l'ensemble de test\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Sauvegarde du modÃ¨le avec pickle\n",
    "model_serialized = pickle.dumps(history)\n",
    "with open('modele_cnn.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ez un modÃ¨le sÃ©quentiel\n",
    "model = models.Sequential()\n",
    "\n",
    "# Couche d'entrÃ©e\n",
    "model.add(Dense(128, input_shape=(12,), activation='relu'))\n",
    "\n",
    "# Couches cachÃ©es profondes\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Couche de sortie\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilez le modÃ¨le\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# RÃ©sumÃ© du modÃ¨le\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce rappel arrÃªtera la formation lorsqu'il n'y aura pas d'amÃ©lioration de la perte de validation pendant 5 Ã©poques consÃ©cutives.  \n",
    "es = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=30,\n",
    "          batch_size=255, \n",
    "          verbose=1, \n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[es])\n",
    "joblib.dump(model, 'model_cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1) \n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(epochs, loss_values, 'blue',label=\"Perte d'entraÃ®nement\")\n",
    "plt.plot(epochs, val_loss_values, 'green', label='Perte de validation')\n",
    "plt.title(\"Lâ€™exactitude et la perte de la classification binaire\")\n",
    "plt.xlabel('Ã‰poques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.savefig('perte.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(epochs, acc, 'blue',label=\"PrÃ©cision de l'entraÃ®nement\")\n",
    "# orange is for \"orange\"\n",
    "plt.plot(epochs, val_acc, 'green', label='PrÃ©cision des validations')\n",
    "plt.title('PrÃ©cision de la formation et de la validation')\n",
    "plt.xlabel('Ã‰poques')\n",
    "plt.ylabel('PrÃ©cision')\n",
    "plt.legend()\n",
    "plt.savefig('precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=250)\n",
    "print(\"Test set accuracy = {} %\".format( results[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "preds = np.round(model.predict(X_test),0)\n",
    "\n",
    "print(confusion_matrix(y_test, preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# CrÃ©ation du modÃ¨le K-means avec un nombre spÃ©cifiÃ© de clusters (k)\n",
    "k = 3  # Remplacez cela par le nombre de clusters que vous souhaitez\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "kmeans.fit(X)\n",
    "\n",
    "# PrÃ©dictions des clusters pour chaque observation\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# CoordonnÃ©es des centroÃ¯des\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Ajout des labels au DataFrame d'origine (si nÃ©cessaire)\n",
    "# Remplacez \"df\" par le nom de votre DataFrame contenant les donnÃ©es\n",
    "df_nv['label'] = labels\n",
    "\n",
    "# Visualisation des clusters\n",
    "# Remplacez \"df\" par le nom de votre DataFrame contenant les donnÃ©es\n",
    "plt.scatter(df_nv['protocol_ICMP'], df_nv['protocol_TCP'], df_nv['protocol_UDP'],c=labels, cmap='viridis', marker='o', edgecolors='black')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('Clustering avec K-means')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame avec les donnÃ©es\n",
    "data = {\n",
    "    'Algorithmes': ['ForÃªt AlÃ©atoire', 'SVM', 'RÃ©gression Logistique', 'Arbre de Decision', 'KNN'],\n",
    "    'Accuracy': [99.94, 95.14, 95, 95.68, 97],\n",
    "    'False Alarm Rate': [None, None, 95, None, None],\n",
    "    'Precision': [98.22, 97.73, 93.60, 97.95, 97.99],\n",
    "    'Recall': [None, None, 95, None, 97],\n",
    "    'F1-score': [None, None, 95, None, 97]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.set_index('Algorithmes').plot(kind='bar', ax=ax, stacked=True)\n",
    "\n",
    "# Ajouter des lÃ©gendes et des Ã©tiquettes\n",
    "plt.title('Performance des Algorithmes')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame avec les donnÃ©es\n",
    "data = {\n",
    "    'Algorithmes': ['ForÃªt AlÃ©atoire', 'SVM', 'RÃ©gression Logistique', 'Arbre de Decision', 'KNN'],\n",
    "    'Precision': [98.22, 97.73, 93.60, 97.95, 97.99]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.set_index('Algorithmes').plot(kind='bar', ax=ax, stacked=True)\n",
    "\n",
    "# Ajouter des lÃ©gendes et des Ã©tiquettes\n",
    "plt.title('Performance des Algorithmes')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le jeu de donnÃ©es\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# SÃ©lectionner les colonnes pertinentes pour le clustering\n",
    "X = df[['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']]\n",
    "\n",
    "# DÃ©finir le nombre de clusters (k)\n",
    "k = 10\n",
    "\n",
    "# CrÃ©er l'instance de l'algorithme K-moyennes\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "# Normaliser les donnÃ©es\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "# EntraÃ®ner l'algorithme sur les donnÃ©es\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Ajouter les Ã©tiquettes de cluster aux donnÃ©es d'origine\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(df)\n",
    "\n",
    "# Pour visualiser, vous pouvez choisir deux caractÃ©ristiques Ã  la fois et les afficher sur un graphique 2D.\n",
    "feature1 = 'pktcount'\n",
    "feature2 = 'bytecount'\n",
    "\n",
    "plt.scatter(df[feature1], df[feature2], c=df['Cluster'], cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, X.columns.get_loc(feature1)], kmeans.cluster_centers_[:, X.columns.get_loc(feature2)], marker='X', s=200, c='red')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title('K-moyennes Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples d'Ã©tiquettes rÃ©elles et de clusters attribuÃ©s\n",
    "print(\"True Labels:\", true_labels[:10])\n",
    "print(\"Cluster Labels:\", cluster_labels[:10])\n",
    "\n",
    "# Afficher les tailles des deux tableaux\n",
    "print(\"Size of True Labels:\", len(true_labels))\n",
    "print(\"Size of Cluster Labels:\", len(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les vÃ©ritables Ã©tiquettes de classe (si disponibles)\n",
    "true_labels = np.array([0, 1])\n",
    "\n",
    "# Les Ã©tiquettes de cluster attribuÃ©es par K-moyennes\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculer les mÃ©triques\n",
    "accuracy = accuracy_score(true_labels, cluster_labels)\n",
    "precision = precision_score(true_labels, cluster_labels, average='weighted')\n",
    "recall = recall_score(true_labels, cluster_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, cluster_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset\n",
    "# Assurez-vous que le chemin du fichier est correct\n",
    "data = pd.read_csv(\"FlowStatsfile.csv\")\n",
    "\n",
    "# SÃ©lectionner les colonnes pertinentes pour le clustering\n",
    "selected_columns = ['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']\n",
    "\n",
    "# CrÃ©er un DataFrame avec les colonnes sÃ©lectionnÃ©es\n",
    "X = data[selected_columns]\n",
    "\n",
    "# Traiter les colonnes catÃ©gorielles (comme les adresses IP et le protocole)\n",
    "#X = pd.get_dummies(X, columns=['ip_src', 'ip_dst', 'eth_type', 'protocole'])\n",
    "\n",
    "# Normaliser les donnÃ©es\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "\n",
    "# DÃ©terminer le nombre optimal de clusters (k) en utilisant la mÃ©thode du coude (Elbow Method)\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Tracer le coude\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('MÃ©thode du coude')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('WCSS')  # Within cluster sum of squares\n",
    "plt.show()\n",
    "\n",
    "# Ã€ partir du graphique du coude, choisissez le nombre optimal de clusters (k)\n",
    "\n",
    "# Appliquer K-means avec le nombre optimal de clusters\n",
    "k = 10  # Remplacez par le nombre optimal de clusters que vous avez choisi\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Ajouter les rÃ©sultats du clustering au DataFrame original\n",
    "data['cluster'] = clusters\n",
    "data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(data[['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour visualiser, vous pouvez choisir deux caractÃ©ristiques Ã  la fois et les afficher sur un graphique 2D.\n",
    "feature1 = 'pktcount'\n",
    "feature2 = 'bytecount'\n",
    "\n",
    "plt.scatter(data[feature1], data[feature2], c=clusters, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, X.columns.get_loc(feature1)], kmeans.cluster_centers_[:, X.columns.get_loc(feature2)], marker='X', s=200, c='red')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title('K-moyennes Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples d'Ã©tiquettes rÃ©elles et de clusters attribuÃ©s\n",
    "print(\"True Labels:\", true_labels[:10])\n",
    "print(\"Cluster Labels:\", cluster_labels[:10])\n",
    "\n",
    "# Afficher les tailles des deux tableaux\n",
    "print(\"Size of True Labels:\", len(true_labels))\n",
    "print(\"Size of Cluster Labels:\", len(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNcXyhttFE0Nl1sptvoUPpV",
   "gpuType": "T4",
   "mount_file_id": "1NIVfK2p5kavmJvjDxPvMJEGHJxqr_rL3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
