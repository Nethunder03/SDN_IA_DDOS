{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24012,
     "status": "ok",
     "timestamp": 1699717969889,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "ZUzpKH4GKDrT",
    "outputId": "f5e1532e-5110-47db-9b45-6da4823df93b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 01:03:36.707309: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, BatchNormalization, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from pyDeepInsight import ImageTransformer, LogScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import pickle, joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1699717970569,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "EkF1hfMSKVW9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('FlowStatsfiles.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1699717970571,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "-mqVmnGSO-_B",
    "outputId": "7ed34302-2da8-483d-f5ba-ca3d45ee305f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datapath</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>in_port</th>\n",
       "      <th>eth_type</th>\n",
       "      <th>eth_src</th>\n",
       "      <th>eth_dst</th>\n",
       "      <th>ip_src</th>\n",
       "      <th>tp_src</th>\n",
       "      <th>ip_dst</th>\n",
       "      <th>...</th>\n",
       "      <th>protocole</th>\n",
       "      <th>pktcount</th>\n",
       "      <th>bytecount</th>\n",
       "      <th>flowdur_sec</th>\n",
       "      <th>flowdur_nsec</th>\n",
       "      <th>pktcount_sec</th>\n",
       "      <th>pktcount_nsec</th>\n",
       "      <th>bytecount_sec</th>\n",
       "      <th>bytecount_nsec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.20172.24.16.10</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>06:ad:7d:c1:dc:1b</td>\n",
       "      <td>0a:b1:b5:23:aa:ad</td>\n",
       "      <td>172.24.16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.1</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>371000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.30172.24.16.40</td>\n",
       "      <td>5</td>\n",
       "      <td>2048</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>769000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>5.981795e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>172.24.16.40172.24.16.30</td>\n",
       "      <td>3</td>\n",
       "      <td>2048</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>758000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.068602e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>172.24.16.30172.24.16.40</td>\n",
       "      <td>3</td>\n",
       "      <td>2048</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>757000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.076618e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.702643e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>172.24.16.40172.24.16.30</td>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>20:77:6d:b7:70:8a</td>\n",
       "      <td>a2:5c:46:f2:34:27</td>\n",
       "      <td>172.24.16.4</td>\n",
       "      <td>0</td>\n",
       "      <td>172.24.16.3</td>\n",
       "      <td>...</td>\n",
       "      <td>ICMP</td>\n",
       "      <td>46</td>\n",
       "      <td>4508</td>\n",
       "      <td>63</td>\n",
       "      <td>752000000</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>6.117021e-08</td>\n",
       "      <td>71.555556</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp  datapath                   flow_id  in_port  eth_type  \\\n",
       "0  1.702643e+09         5  172.24.16.20172.24.16.10        1      2048   \n",
       "1  1.702643e+09         5  172.24.16.30172.24.16.40        5      2048   \n",
       "2  1.702643e+09         5  172.24.16.40172.24.16.30        3      2048   \n",
       "3  1.702643e+09         2  172.24.16.30172.24.16.40        3      2048   \n",
       "4  1.702643e+09         2  172.24.16.40172.24.16.30        2      2048   \n",
       "\n",
       "             eth_src            eth_dst       ip_src  tp_src       ip_dst  \\\n",
       "0  06:ad:7d:c1:dc:1b  0a:b1:b5:23:aa:ad  172.24.16.2       0  172.24.16.1   \n",
       "1  a2:5c:46:f2:34:27  20:77:6d:b7:70:8a  172.24.16.3       0  172.24.16.4   \n",
       "2  20:77:6d:b7:70:8a  a2:5c:46:f2:34:27  172.24.16.4       0  172.24.16.3   \n",
       "3  a2:5c:46:f2:34:27  20:77:6d:b7:70:8a  172.24.16.3       0  172.24.16.4   \n",
       "4  20:77:6d:b7:70:8a  a2:5c:46:f2:34:27  172.24.16.4       0  172.24.16.3   \n",
       "\n",
       "   ...  protocole  pktcount  bytecount flowdur_sec  flowdur_nsec  \\\n",
       "0  ...       ICMP         1         98           0     371000000   \n",
       "1  ...       ICMP        46       4508          63     769000000   \n",
       "2  ...       ICMP        46       4508          63     758000000   \n",
       "3  ...       ICMP        46       4508          63     757000000   \n",
       "4  ...       ICMP        46       4508          63     752000000   \n",
       "\n",
       "   pktcount_sec  pktcount_nsec  bytecount_sec  bytecount_nsec  label  \n",
       "0      0.000000   0.000000e+00       0.000000        0.000000      0  \n",
       "1      0.730159   5.981795e-08      71.555556        0.000006      0  \n",
       "2      0.730159   6.068602e-08      71.555556        0.000006      0  \n",
       "3      0.730159   6.076618e-08      71.555556        0.000006      0  \n",
       "4      0.730159   6.117021e-08      71.555556        0.000006      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136766 entries, 0 to 136765\n",
      "Data columns (total 23 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   timestamp       136766 non-null  float64\n",
      " 1   datapath        136766 non-null  int64  \n",
      " 2   flow_id         136766 non-null  object \n",
      " 3   in_port         136766 non-null  int64  \n",
      " 4   eth_type        136766 non-null  int64  \n",
      " 5   eth_src         136766 non-null  object \n",
      " 6   eth_dst         136766 non-null  object \n",
      " 7   ip_src          136766 non-null  object \n",
      " 8   tp_src          136766 non-null  int64  \n",
      " 9   ip_dst          136766 non-null  object \n",
      " 10  tp_dst          136766 non-null  int64  \n",
      " 11  icmp_code       136766 non-null  int64  \n",
      " 12  icmp_type       136766 non-null  int64  \n",
      " 13  protocole       136766 non-null  object \n",
      " 14  pktcount        136766 non-null  int64  \n",
      " 15  bytecount       136766 non-null  int64  \n",
      " 16  flowdur_sec     136766 non-null  int64  \n",
      " 17  flowdur_nsec    136766 non-null  int64  \n",
      " 18  pktcount_sec    136766 non-null  float64\n",
      " 19  pktcount_nsec   136766 non-null  float64\n",
      " 20  bytecount_sec   136766 non-null  float64\n",
      " 21  bytecount_nsec  136766 non-null  float64\n",
      " 22  label           136766 non-null  int64  \n",
      "dtypes: float64(5), int64(12), object(6)\n",
      "memory usage: 24.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1699717970573,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "kN4z-xCKPHGh"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop(['timestamp','datapath','flow_id','in_port','eth_type','eth_src', 'eth_dst', 'ip_src', 'ip_dst', 'protocole', 'icmp_code', 'icmp_type', 'flowdur_sec', 'flowdur_nsec'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1699717971090,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "Rw9mKgcTPHTg"
   },
   "outputs": [],
   "source": [
    "df.protocole.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['label']\n",
    "del df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1699717971091,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "zRI5c5zhPHgd",
    "outputId": "da278e6c-46cc-40e9-db89-d5569cfbd15f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp_src</th>\n",
       "      <th>tp_dst</th>\n",
       "      <th>pktcount</th>\n",
       "      <th>bytecount</th>\n",
       "      <th>pktcount_sec</th>\n",
       "      <th>pktcount_nsec</th>\n",
       "      <th>bytecount_sec</th>\n",
       "      <th>bytecount_nsec</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.083276e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.686728e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.229505e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.711205e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.276371e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.713466e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.280699e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.878307e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.724859e-07</td>\n",
       "      <td>2.229147e-08</td>\n",
       "      <td>3.302512e-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tp_src  tp_dst  pktcount     bytecount  pktcount_sec  pktcount_nsec  \\\n",
       "0     0.0     0.0  0.000002  4.083276e-09      0.000000   0.000000e+00   \n",
       "1     0.0     0.0  0.000079  1.878307e-07      0.000006   1.686728e-07   \n",
       "2     0.0     0.0  0.000079  1.878307e-07      0.000006   1.711205e-07   \n",
       "3     0.0     0.0  0.000079  1.878307e-07      0.000006   1.713466e-07   \n",
       "4     0.0     0.0  0.000079  1.878307e-07      0.000006   1.724859e-07   \n",
       "\n",
       "   bytecount_sec  bytecount_nsec  label  \n",
       "0   0.000000e+00    0.000000e+00    0.0  \n",
       "1   2.229147e-08    3.229505e-10    0.0  \n",
       "2   2.229147e-08    3.276371e-10    0.0  \n",
       "3   2.229147e-08    3.280699e-10    0.0  \n",
       "4   2.229147e-08    3.302512e-10    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "df_nrm= scaler.fit_transform(df)\n",
    "df_nv = pd.DataFrame(df_nrm, columns=df.columns)\n",
    "df_nv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1699717971091,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "eS-hehCVPHr1"
   },
   "outputs": [],
   "source": [
    "#X = df_nv.iloc[:,1:17].astype(float)\n",
    "#y = df_nv.iloc[:,-1]\n",
    "# X = df_nv.drop('label', axis=1).astype(float)\n",
    "# y = df_nv['label']\n",
    "X = df.drop('label', axis=1).astype(float)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1699717971092,
     "user": {
      "displayName": "Parfait EMENT",
      "userId": "02662244385463089072"
     },
     "user_tz": 0
    },
    "id": "g-GT8Gp6QVTE"
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgTTVUBxQebM",
    "outputId": "b6789956-fd89-406f-d53c-a91f98ad957e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Précision sur l'ensemble de validation : 99.56%\n",
      "Précision sur l'ensemble de test : 99.6%\n",
      "Meilleurs hyperparamètres: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10232\n",
      "           1       1.00      0.99      1.00     10283\n",
      "\n",
      "    accuracy                           1.00     20515\n",
      "   macro avg       1.00      1.00      1.00     20515\n",
      "weighted avg       1.00      1.00      1.00     20515\n",
      "\n",
      "1.18 Minute(s) --- temps pour le modèle KNN \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KNN.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# GridSearchCV pour trouver les meilleurs hyperparamètres\n",
    "knnc = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knnc_search = GridSearchCV(knnc, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy', verbose=2)\n",
    "knnc_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = knnc_search.best_params_\n",
    "n_neighbors = best_params['n_neighbors']\n",
    "weights = best_params['weights']\n",
    "metric = best_params['metric']\n",
    "\n",
    "# Utilisation des meilleurs hyperparamètres sur l'ensemble d'entraînement\n",
    "KNN = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, weights=weights)\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de validation\n",
    "predicted_val = KNN.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"Précision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de test\n",
    "predicted_test = KNN.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "print(f\"Précision sur l'ensemble de test : {round(accuracy_test * 100, 2)}%\")\n",
    "\n",
    "# Affichage des résultats finaux\n",
    "print(\"Meilleurs hyperparamètres:\", best_params)\n",
    "print(classification_report(predicted_test, y_test))\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minute(s) --- temps pour le modèle KNN \")\n",
    "\n",
    "    \n",
    "joblib.dump(KNN, 'KNN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMj3aTZUYiEo"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# Liste pour stocker les précisions pour chaque noyau\n",
    "accuracy_list = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Création du modèle SVM avec le noyau actuel\n",
    "    SVM = svm.SVC(kernel=kernel)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    SVM.fit(X_train, y_train)\n",
    "\n",
    "    # Prédiction et évaluation sur l'ensemble de validation\n",
    "    predicted_svm = SVM.predict(X_val)\n",
    "    accuracy_val = accuracy_score(y_val, predicted_svm)\n",
    "    print(f\"Précision sur l'ensemble de validation du noyau {kernel} : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "    #Prédiction et évaluation sur l'ensemble de test\n",
    "    predicted_test = SVM.predict(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, predicted_test)\n",
    "    accuracy_list.append(accuracy_test)\n",
    "    print(f\"Précision sur l'ensemble de test du noyau {kernel} : {round(accuracy_test * 100, 2)}%\")\n",
    "    print(\"########################################################################\")\n",
    "\n",
    "# Sélection du meilleur noyau en fonction de la précision maximale\n",
    "best_kernel = kernels[np.argmax(accuracy_list)]\n",
    "\n",
    "# Entraînement final avec le meilleur noyau\n",
    "best_SVM = svm.SVC(kernel=best_kernel)\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction avec le modèle optimal\n",
    "predicted_svm = best_SVM.predict(X_test)\n",
    "\n",
    "# Calcul de la précision\n",
    "accuracy_svm = accuracy_score(y_test, predicted_svm)\n",
    "\n",
    "# Affichage des résultats finaux\n",
    "print(f\"Précision du modèle SVM avec le meilleur noyau (noyau {best_kernel}): {round(accuracy_svm * 100, 2)}%\")\n",
    "print(\"########################################################################\")\n",
    "print('Le meilleur noyau est:', best_kernel)\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(y_test, predicted_svm))\n",
    "print(\"########################################################################\")\n",
    "print(\"--- %s seconds --- temps pour le model SVM\" % (time.time() - start_time))\n",
    "\n",
    "# Sauvegarde du modèle avec pickle\n",
    "model_serialized = pickle.dumps(best_SVM)\n",
    "with open('modele_svm.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "\n",
    "joblib.dump(best_SVM, 'best_SVM.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNMHRu51ioTy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Définissez la grille d'hyperparamètres à explorer\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Créez le modèle RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Utilisez GridSearchCV pour trouver les meilleurs hyperparamètres\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtenir les meilleurs hyperparamètres trouvés\n",
    "rf_classifier_best = grid_search.best_estimator_\n",
    "\n",
    "# Entraînez un modèle RandomForest avec les meilleurs hyperparamètres\n",
    "rf_classifier_best.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de validation\n",
    "predicted_val = rf_classifier_best.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"Précision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# Prédisez les étiquettes\n",
    "predicted_rf = rf_classifier_best.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, predicted_rf)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"Random Forest Précision: {round(accuracy_rf * 100, 2)}%\")\n",
    "print(classification_report(predicted_rf, y_test))\n",
    "print(\"--- %s secondes --- temps pour le Random Forest \" % (time.time() - start_time))\n",
    "\n",
    "# Sauvegarde du modèle avec pickle\n",
    "model_serialized = pickle.dumps(rf_classifier_best)\n",
    "with open('modele_rf.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(rf_classifier_best, 'rf_classifier_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V2F7WcuRmzQi",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aizen/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:457: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/aizen/anaconda3/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/aizen/anaconda3/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision sur l'ensemble de validation : 99.28%\n",
      "Précision: 99.29%\n",
      "\n",
      "########################################################################\n",
      "Meilleur solveur : lbfgs\n",
      "########################################################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     10251\n",
      "           1       1.00      0.99      0.99     10264\n",
      "\n",
      "    accuracy                           0.99     20515\n",
      "   macro avg       0.99      0.99      0.99     20515\n",
      "weighted avg       0.99      0.99      0.99     20515\n",
      " \n",
      "\n",
      "########################################################################\n",
      "28.74 Minutes --- temps pour la régression logistique \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aizen/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paramètres du modèle\n",
    "C_value = 0.03\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "results_lr = []\n",
    "\n",
    "start_time = time.time()\n",
    "best_accuracy = 0\n",
    "best_solver = None\n",
    "\n",
    "for solver in solvers:\n",
    "    # Entraînement du modèle\n",
    "    logreg_model = LogisticRegression(C=C_value, solver=solver, max_iter=1000, tol=1e-3).fit(X_train, y_train)\n",
    "    predicted_lr = logreg_model.predict(X_test)\n",
    "    accuracy_lr = accuracy_score(y_test, predicted_lr)\n",
    "    \n",
    "    results_lr.append({\n",
    "        'solver': solver,\n",
    "        'accuracy': f'{accuracy_lr * 100:.2f}%',\n",
    "        'coefficients': {'W': logreg_model.coef_, 'b': logreg_model.intercept_}\n",
    "    })\n",
    "\n",
    "    if accuracy_lr > best_accuracy:\n",
    "        best_accuracy = accuracy_lr\n",
    "        best_solver = solver\n",
    "\n",
    "# Entraînement final avec le meilleur solveur\n",
    "logreg_model = LogisticRegression(C=C_value, solver=best_solver).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de validation\n",
    "predicted_val = logreg_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"Précision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "# Prédisez les étiquettes\n",
    "\n",
    "predicted_lr = logreg_model.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, predicted_lr)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"Précision: {accuracy_lr * 100:.2f}%\\n\")\n",
    "print(\"########################################################################\")\n",
    "print('Meilleur solveur :', best_solver)\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(predicted_lr, y_test), '\\n')\n",
    "print(\"########################################################################\")\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minutes --- temps pour la régression logistique \")\n",
    "#print(f\"--- {(time.time() - start_time:.2f)} Minutes --- temps pour la régression logistique\")\n",
    "\n",
    "# Sauvegarde du modèle avec pickle\n",
    "model_serialized = pickle.dumps(logreg_model)\n",
    "with open('modele_lr.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(logreg_model, 'logreg_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Précision sur l'ensemble de validation : 99.43%\n",
      "criterion: gini, max depth: None, max_leaf: 5\n",
      "La précision est : 99.5%\n",
      "########################################################################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10251\n",
      "           1       1.00      0.99      1.00     10264\n",
      "\n",
      "    accuracy                           1.00     20515\n",
      "   macro avg       1.00      1.00      1.00     20515\n",
      "weighted avg       1.00      1.00      1.00     20515\n",
      "\n",
      "########################################################################\n",
      "0.26 Minutes secondes --- temps pour l'arbre de décision \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dt_classifier.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=gini, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=2; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ......criterion=gini, max_depth=3, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ......criterion=gini, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=entropy, max_depth=None, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=2, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=3, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=4, max_leaf_nodes=5; total time=   0.3s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ...criterion=entropy, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END criterion=log_loss, max_depth=None, max_leaf_nodes=5; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=2, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=3; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=4; total time=   0.3s\n",
      "[CV] END ..criterion=log_loss, max_depth=3, max_leaf_nodes=5; total time=   0.4s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=4; total time=   0.2s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=4, max_leaf_nodes=5; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=2; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=3; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=4; total time=   0.1s\n",
      "[CV] END ..criterion=log_loss, max_depth=5, max_leaf_nodes=5; total time=   0.1s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Recherche des hyperparamètres optimaux\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'max_leaf_nodes': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "dt_search = GridSearchCV(dt_classifier, param_grid=param_grid, n_jobs=-1, cv=5, scoring='accuracy', verbose=2)\n",
    "dt_search.fit(X_train, y_train)\n",
    "\n",
    "criterion = dt_search.best_params_['criterion']\n",
    "max_depth = dt_search.best_params_['max_depth']\n",
    "max_leaf_nodes = dt_search.best_params_['max_leaf_nodes']\n",
    "\n",
    "# Entraînement du modèle avec les hyperparamètres optimaux\n",
    "dt_classifier = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction et évaluation sur l'ensemble de validation\n",
    "predicted_val = dt_classifier.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "print(f\"Précision sur l'ensemble de validation : {round(accuracy_val * 100, 2)}%\")\n",
    "\n",
    "\n",
    "# Prédiction et évaluation du modèle\n",
    "predicted_dt = dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, predicted_dt)\n",
    "\n",
    "# Stockage des résultats\n",
    "results = {\n",
    "    'criterion': criterion,\n",
    "    'max_depth': max_depth,\n",
    "    'max_leaf_nodes': max_leaf_nodes,\n",
    "    'accuracy': round(accuracy_dt * 100, 2)\n",
    "}\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"criterion: {criterion}, max depth: {max_depth}, max_leaf: {max_leaf_nodes}\")\n",
    "print(f\"La précision est : {results['accuracy']}%\")\n",
    "print(\"########################################################################\")\n",
    "print(classification_report(predicted_dt, y_test))\n",
    "print(\"########################################################################\")\n",
    "\n",
    "print(f\"{((time.time()/60) - (start_time)/60):.2f} Minutes secondes --- temps pour l'arbre de décision \")\n",
    "\n",
    "# Sauvegarde du modèle avec pickle\n",
    "model_serialized = pickle.dumps(dt_classifier)\n",
    "with open('modele_dt.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)\n",
    "    \n",
    "joblib.dump(dt_classifier, 'dt_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle CNN 1D\n",
    "model = models.Sequential()\n",
    "\n",
    "# Ajout d'une couche de convolution 1D avec activation ReLU et pooling\n",
    "model.add(layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(15, 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Ajout d'une deuxième couche de convolution 1D avec activation ReLU et pooling\n",
    "model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Aplatir les données pour les passer à une couche dense (fully connected)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Ajout d'une couche fully connected avec activation ReLU\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Ajout de la couche de sortie\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Utilisez 'softmax' si vous avez plus de deux classes\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Utilisez 'categorical_crossentropy' si vous avez plus de deux classes\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un rappel qui arrête la formation lorsqu'il n'y aura pas d'amélioration de la perte de validation pendant 5 époques consécutives.\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True)\n",
    "              \n",
    "# Entraînement du modèle avec ensemble de validation\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val), callbacks=[es])\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Sauvegarde du modèle avec pickle\n",
    "model_serialized = pickle.dumps(history)\n",
    "with open('modele_cnn.pickle', 'wb') as model_file:\n",
    "    model_file.write(model_serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez un modèle séquentiel\n",
    "model = models.Sequential()\n",
    "\n",
    "# Couche d'entrée\n",
    "model.add(Dense(128, input_shape=(12,), activation='relu'))\n",
    "\n",
    "# Couches cachées profondes\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Couche de sortie\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilez le modèle\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce rappel arrêtera la formation lorsqu'il n'y aura pas d'amélioration de la perte de validation pendant 5 époques consécutives.  \n",
    "es = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='Adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          epochs=30,\n",
    "          batch_size=255, \n",
    "          verbose=1, \n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[es])\n",
    "joblib.dump(model, 'model_cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1) \n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(epochs, loss_values, 'blue',label=\"Perte d'entraînement\")\n",
    "plt.plot(epochs, val_loss_values, 'green', label='Perte de validation')\n",
    "plt.title(\"L’exactitude et la perte de la classification binaire\")\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.savefig('perte.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(epochs, acc, 'blue',label=\"Précision de l'entraînement\")\n",
    "# orange is for \"orange\"\n",
    "plt.plot(epochs, val_acc, 'green', label='Précision des validations')\n",
    "plt.title('Précision de la formation et de la validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "plt.savefig('precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=250)\n",
    "print(\"Test set accuracy = {} %\".format( results[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "preds = np.round(model.predict(X_test),0)\n",
    "\n",
    "print(confusion_matrix(y_test, preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Création du modèle K-means avec un nombre spécifié de clusters (k)\n",
    "k = 3  # Remplacez cela par le nombre de clusters que vous souhaitez\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Prédictions des clusters pour chaque observation\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Coordonnées des centroïdes\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Ajout des labels au DataFrame d'origine (si nécessaire)\n",
    "# Remplacez \"df\" par le nom de votre DataFrame contenant les données\n",
    "df_nv['label'] = labels\n",
    "\n",
    "# Visualisation des clusters\n",
    "# Remplacez \"df\" par le nom de votre DataFrame contenant les données\n",
    "plt.scatter(df_nv['protocol_ICMP'], df_nv['protocol_TCP'], df_nv['protocol_UDP'],c=labels, cmap='viridis', marker='o', edgecolors='black')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('Clustering avec K-means')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un DataFrame avec les données\n",
    "data = {\n",
    "    'Algorithmes': ['Forêt Aléatoire', 'SVM', 'Régression Logistique', 'Arbre de Decision', 'KNN'],\n",
    "    'Accuracy': [99.94, 95.14, 95, 95.68, 97],\n",
    "    'False Alarm Rate': [None, None, 95, None, None],\n",
    "    'Precision': [98.22, 97.73, 93.60, 97.95, 97.99],\n",
    "    'Recall': [None, None, 95, None, 97],\n",
    "    'F1-score': [None, None, 95, None, 97]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.set_index('Algorithmes').plot(kind='bar', ax=ax, stacked=True)\n",
    "\n",
    "# Ajouter des légendes et des étiquettes\n",
    "plt.title('Performance des Algorithmes')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un DataFrame avec les données\n",
    "data = {\n",
    "    'Algorithmes': ['Forêt Aléatoire', 'SVM', 'Régression Logistique', 'Arbre de Decision', 'KNN'],\n",
    "    'Precision': [98.22, 97.73, 93.60, 97.95, 97.99]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.set_index('Algorithmes').plot(kind='bar', ax=ax, stacked=True)\n",
    "\n",
    "# Ajouter des légendes et des étiquettes\n",
    "plt.title('Performance des Algorithmes')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le jeu de données\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Sélectionner les colonnes pertinentes pour le clustering\n",
    "X = df[['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']]\n",
    "\n",
    "# Définir le nombre de clusters (k)\n",
    "k = 10\n",
    "\n",
    "# Créer l'instance de l'algorithme K-moyennes\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "# Entraîner l'algorithme sur les données\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Ajouter les étiquettes de cluster aux données d'origine\n",
    "df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Afficher les résultats\n",
    "print(df)\n",
    "\n",
    "# Pour visualiser, vous pouvez choisir deux caractéristiques à la fois et les afficher sur un graphique 2D.\n",
    "feature1 = 'pktcount'\n",
    "feature2 = 'bytecount'\n",
    "\n",
    "plt.scatter(df[feature1], df[feature2], c=df['Cluster'], cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, X.columns.get_loc(feature1)], kmeans.cluster_centers_[:, X.columns.get_loc(feature2)], marker='X', s=200, c='red')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title('K-moyennes Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples d'étiquettes réelles et de clusters attribués\n",
    "print(\"True Labels:\", true_labels[:10])\n",
    "print(\"Cluster Labels:\", cluster_labels[:10])\n",
    "\n",
    "# Afficher les tailles des deux tableaux\n",
    "print(\"Size of True Labels:\", len(true_labels))\n",
    "print(\"Size of Cluster Labels:\", len(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les véritables étiquettes de classe (si disponibles)\n",
    "true_labels = np.array([0, 1])\n",
    "\n",
    "# Les étiquettes de cluster attribuées par K-moyennes\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Calculer les métriques\n",
    "accuracy = accuracy_score(true_labels, cluster_labels)\n",
    "precision = precision_score(true_labels, cluster_labels, average='weighted')\n",
    "recall = recall_score(true_labels, cluster_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, cluster_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset\n",
    "# Assurez-vous que le chemin du fichier est correct\n",
    "data = pd.read_csv(\"FlowStatsfile.csv\")\n",
    "\n",
    "# Sélectionner les colonnes pertinentes pour le clustering\n",
    "selected_columns = ['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']\n",
    "\n",
    "# Créer un DataFrame avec les colonnes sélectionnées\n",
    "X = data[selected_columns]\n",
    "\n",
    "# Traiter les colonnes catégorielles (comme les adresses IP et le protocole)\n",
    "#X = pd.get_dummies(X, columns=['ip_src', 'ip_dst', 'eth_type', 'protocole'])\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "\n",
    "# Déterminer le nombre optimal de clusters (k) en utilisant la méthode du coude (Elbow Method)\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Tracer le coude\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Méthode du coude')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('WCSS')  # Within cluster sum of squares\n",
    "plt.show()\n",
    "\n",
    "# À partir du graphique du coude, choisissez le nombre optimal de clusters (k)\n",
    "\n",
    "# Appliquer K-means avec le nombre optimal de clusters\n",
    "k = 10  # Remplacez par le nombre optimal de clusters que vous avez choisi\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Ajouter les résultats du clustering au DataFrame original\n",
    "data['cluster'] = clusters\n",
    "data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Afficher les résultats\n",
    "print(data[['pktcount', 'bytecount', 'flowdur_sec', 'flowdur_nsec', 'pktcount_sec', 'pktcount_nsec', 'bytecount_sec', 'bytecount_nsec']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour visualiser, vous pouvez choisir deux caractéristiques à la fois et les afficher sur un graphique 2D.\n",
    "feature1 = 'pktcount'\n",
    "feature2 = 'bytecount'\n",
    "\n",
    "plt.scatter(data[feature1], data[feature2], c=clusters, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, X.columns.get_loc(feature1)], kmeans.cluster_centers_[:, X.columns.get_loc(feature2)], marker='X', s=200, c='red')\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title('K-moyennes Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques exemples d'étiquettes réelles et de clusters attribués\n",
    "print(\"True Labels:\", true_labels[:10])\n",
    "print(\"Cluster Labels:\", cluster_labels[:10])\n",
    "\n",
    "# Afficher les tailles des deux tableaux\n",
    "print(\"Size of True Labels:\", len(true_labels))\n",
    "print(\"Size of Cluster Labels:\", len(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNcXyhttFE0Nl1sptvoUPpV",
   "gpuType": "T4",
   "mount_file_id": "1NIVfK2p5kavmJvjDxPvMJEGHJxqr_rL3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
